{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba074ee4",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c2fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from saltup.ai.classification.datagenerator import ClassificationDataloader, pytorch_ClassificationDataGenerator\n",
    "from saltup.ai.base_dataformat.base_datagen import *\n",
    "from saltup.ai.training.app_callbacks import ClassificationEvaluationsCallback\n",
    "from saltup.ai.classification.evaluate import evaluate_model\n",
    "from saltup.utils.jupyter_notebook import generate_notebook_id, save_current_notebook\n",
    "from saltup.ai.training.train import training\n",
    "from saltup.utils.data.image.image_utils import Image, ColorMode\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader as pytorch_DataGenerator\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the dataset\n",
    "!wget --no-check-certificate https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip -O cats_and_dogs.zip\n",
    "\n",
    "# Unzip the file using Python\n",
    "with zipfile.ZipFile(\"cats_and_dogs.zip\", \"r\") as zip_ref:\n",
    "    extract_dir = \"dataset\"\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "    zip_ref.extractall(extract_dir)\n",
    "    \n",
    "    os.remove(\"cats_and_dogs.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9512bae",
   "metadata": {},
   "source": [
    "# Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174636d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 32\n",
    "INPUT_SIZE = (128, 128)\n",
    "EPOCHS = 4\n",
    "CALLBACK_EPOCH = 3  # Test the model every CALLBACK_EPOCH epochs\n",
    "TRAIN_DATA_DIR = './dataset/cats_and_dogs_filtered/train'\n",
    "TEST_DATA_DIR = './dataset/cats_and_dogs_filtered/validation'\n",
    "CLASS_NAMES = ['cats', 'dogs']\n",
    "CLASS_DICTIONARY = {'cats': 0, 'dogs': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c935964a",
   "metadata": {},
   "source": [
    "# Load a pre-trained model or define your own architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ba91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import keras\n",
    "#Define the model architecture\n",
    "\n",
    "class CNN2Class(nn.Module):\n",
    "    def __init__(self, input_shape=(3, 128, 128), num_classes=NUM_CLASSES):\n",
    "        super(CNN2Class, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 4, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(3)\n",
    "        self.bn1 = nn.BatchNorm2d(4)\n",
    "        self.drop1 = nn.Dropout(0.1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(4, 8, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(3)\n",
    "        self.bn2 = nn.BatchNorm2d(8)\n",
    "        self.drop2 = nn.Dropout(0.1)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(3)\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        self.drop3 = nn.Dropout(0.1)\n",
    "\n",
    "        # Calculate the correct flattened dimension after convolutions and pooling\n",
    "        # Input: (3, 128, 128)\n",
    "        # After conv1 + pool1: (4, 42, 42)\n",
    "        # After conv2 + pool2: (8, 14, 14)\n",
    "        # After conv3 + pool3: (16, 4, 4)\n",
    "        self.flat_dim = 16 * 4 * 4\n",
    "        self.fc = nn.Linear(self.flat_dim, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.drop1(x)\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.drop2(x)\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.drop3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "\n",
    "def build_model(input_shape=(128, 128, 3), num_classes=2):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = keras.layers.Conv2D(8, (3, 3), activation='relu')(inputs)\n",
    "    x = keras.layers.MaxPooling2D()(x)\n",
    "    x = keras.layers.Conv2D(16, (3, 3), activation='relu')(x)\n",
    "    x = keras.layers.MaxPooling2D()(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(64, activation='relu')(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c0eb6",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c92277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the preprocessing function\n",
    "\n",
    "def preprocess(image:np.ndarray, target_size:tuple) -> np.ndarray:\n",
    "    \"\"\"Preprocess the image by resizing and normalizing.\"\"\"\n",
    "    temp_image = Image(image)\n",
    "    temp_image = temp_image.resize(target_size)\n",
    "    img = temp_image.get_data()\n",
    "    img = img / 255.0  # Normalize pixel values between [0, 1]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978336e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the augmentation transformations\n",
    "transformed_img = A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.5)\n",
    "   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90974b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the data loaders\n",
    "\n",
    "train_dataloader = ClassificationDataloader(\n",
    "    source=TRAIN_DATA_DIR,\n",
    "    classes_dict=CLASS_DICTIONARY,\n",
    "    img_size=(224, 224, 3)\n",
    ")\n",
    "\n",
    "test_dataloader = ClassificationDataloader(\n",
    "    source=TEST_DATA_DIR,\n",
    "    classes_dict=CLASS_DICTIONARY,\n",
    "    img_size=(224, 224, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Classification Data Generator\n",
    "\n",
    "train_gen = pytorch_ClassificationDataGenerator(\n",
    "    dataloader=train_dataloader,\n",
    "    target_size=INPUT_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    preprocess=preprocess,\n",
    "    transform=transformed_img\n",
    ")\n",
    "\n",
    "\n",
    "test_gen = pytorch_ClassificationDataGenerator(\n",
    "    dataloader=test_dataloader,\n",
    "    target_size=INPUT_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    preprocess=preprocess,\n",
    "    transform=None  # no augmentation\n",
    ")\n",
    "\n",
    "callback_test_data = pytorch_DataGenerator(test_gen, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "images, labels = next(iter(train_gen))\n",
    "print(\"image shape\", images.shape)\n",
    "print(\"label shape\", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bfb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single example image and its label\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_image(image, label):\n",
    "    # If image is a torch tensor, convert to numpy\n",
    "    if hasattr(image, 'detach'):\n",
    "        image = image.detach().cpu().numpy()\n",
    "    # If image has shape (C, H, W), transpose to (H, W, C)\n",
    "    if image.ndim == 3 and image.shape[0] in [1, 3]:\n",
    "        image = image.transpose(1, 2, 0)\n",
    "    plt.imshow(image.squeeze())\n",
    "    plt.title(f\"Label: {label}\")\n",
    "    #plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "example_image = train_gen[0][0][0]  # Get the first image from the first batch\n",
    "example_label = train_gen[0][1][0]  # Get the corresponding label\n",
    "print(f\"Example image shape: {example_image.shape}\")\n",
    "plot_image(example_image, example_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bc1de",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad37a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the output directory and create it if it doesn't exist\n",
    "todaytime = datetime.now()\n",
    "output_dir = \"./training_outputs\"\n",
    "current_tests_folder_name = \"train_{}\".format(todaytime.strftime(\"%d-%m-%Y_%H-%M-%S\"))\n",
    "current_output_dir = os.path.join(output_dir, current_tests_folder_name)\n",
    "if not os.path.exists(current_output_dir):\n",
    "    os.makedirs(current_output_dir)\n",
    "    \n",
    "\n",
    "custom_cb = ClassificationEvaluationsCallback(\n",
    "    datagen=callback_test_data,\n",
    "    end_of_train_datagen=callback_test_data,\n",
    "    every_epoch=CALLBACK_EPOCH,\n",
    "    output_file=os.path.join(current_output_dir, \"classification_evaluations.txt\"),\n",
    "    class_names=CLASS_NAMES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f700a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model compilation parameters\n",
    "torch_model = CNN2Class(num_classes=NUM_CLASSES)\n",
    "\n",
    "#model = build_model(input_shape=(128, 128, 3), num_classes=NUM_CLASSES)\n",
    "optimizer =  Adam(torch_model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Define k-fold parameters\n",
    "# This is set to False for simplicity, but can be enabled for k-fold cross-validation\n",
    "# If enabled, the split parameter defines the proportion of data in each fold\n",
    "kfold_parameters = {'enable':False, 'split':[0.2, 0.2, 0.2, 0.2, 0.2]}\n",
    "\n",
    "# Define the model output name\n",
    "model_output_name = \"tiny_model\"\n",
    "\n",
    "# Start the training process\n",
    "results_dict = training(\n",
    "        train_gen,\n",
    "        model=torch_model,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer,\n",
    "        epochs=EPOCHS,\n",
    "        output_dir=current_output_dir,\n",
    "        validation_split=[0.2, 0.8],\n",
    "        kfold_param =kfold_parameters,\n",
    "        model_output_name = model_output_name,\n",
    "        training_callback=[custom_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx2keras import onnx_to_keras\n",
    "\n",
    "# Load ONNX model\n",
    "onnx_model = onnx.load('./training_outputs/train_05-09-2025_16-44-06/saved_models/tiny_model_best.onnx')\n",
    "\n",
    "# Now convert to Keras\n",
    "k_model = onnx_to_keras(onnx_model, ['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3071ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model = torch.jit.script(torch_model)\n",
    "scripted_model.save(\"model.pt\")\n",
    "\n",
    "torch.save(torch_model.state_dict(), \"model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_torch = torch.load(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4612ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a8a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.rand(3, 128, 128)\n",
    "\n",
    "output = torch_model(train_gen[0][0].unsqueeze(0))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc947fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a377ae9",
   "metadata": {},
   "source": [
    "# Inference on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results and evaluate the model\n",
    "#model_path = [res for res in results_dict['models_paths'] if res.endswith('.onnx')][0]\n",
    "\n",
    "model_path = \"model.pt\"\n",
    "\n",
    "global_metric, metric_per_class = evaluate_model(\n",
    "    model_path, \n",
    "    test_gen=callback_test_data,\n",
    "    output_dir=current_output_dir,\n",
    "    conf_matrix=True\n",
    ")\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Global metrics:\")\n",
    "print(\"FP:\", global_metric.getFP())\n",
    "print(\"FN:\", global_metric.getFN())\n",
    "print(\"Accuracy:\", global_metric.getAccuracy())\n",
    "\n",
    "print(\"\\nPer-class metrics:\")\n",
    "for idx, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"Class: {class_name}\")\n",
    "    print(\"  FP:\", metric_per_class[idx].getFP())\n",
    "    print(\"  FN:\", metric_per_class[idx].getFN())\n",
    "    print(\"  Accuracy:\", metric_per_class[idx].getAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current notebook with the results. This is done at the end to ensure all outputs are captured.\n",
    "save_current_notebook(current_output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saltup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
